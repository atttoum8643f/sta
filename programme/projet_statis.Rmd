---
title: "Projet STATIS: Réponse feuille de route"
output: pdf_document
---
## I-Préliminaires pour la situation 1

### 1.1 Produit scalaire 

La matrice diagonale des poids des \( n \) individus est \( W \) (par défaut, \( W = \frac{1}{n} I_n \)).
On considérera également une matrice diagonale des poids des \( p \) colonnes : \( C = \frac{1}{p} I_p \) par défaut.

1. Le produit scalaire entre deux matrices \( A \) et \( B \) de taille \( (n, p) \) est :

\[ [A|B] = \text{tr}(CA'WB) \]

La norme d'une matrice \( A \) correspondant à ce produit scalaire sera notée \( \left[\left| A \right|\right] \).

a) Ecrivons le produit scalaire sous forme \(tr(\tilde{A}'\tilde{B})\) (produit scalaire de Frobénius) en explicitant la transformation \(Z \quad en \quad \tilde{Z}\), \( \forall Z \) \( (n, p) \).

Soit \(A,B \in M_{n,p}(\mathbb{R}) \)

\begin{align*}
[A \mid B] &= \text{tr}(CA'WB) \\
&= \text{tr}(C^{\frac{1}{2}} A'W^{\frac{1}{2}} W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}((W^{\frac{1}{2}} A C^{\frac{1}{2}})' W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}(\tilde{A}'\tilde{B})
\end{align*}

où \(\forall (n,p)\) \(\tilde{Z} =  W^{\frac{1}{2}} Z C^{\frac{1}{2}}\)

b) Pour montrer que \([A \mid B] = \text{tr}(\tilde{A}'\tilde{B})\) est un produit scalaire, nous devons démontrer les propriétés suivantes : 

**(i) Symétrie** : \(\forall\) \(A,B \in M_{n,p}(\mathbb{R}) \)

\[
[A \mid B] = [B \mid A]
\]

**(ii) Linéarité** :\(\forall\) \(A, B_1, B_2 \in M_{n,p}(\mathbb{R}) \) et \(\alpha, \beta in \mathbb{R}\)

\[
[A \mid (\alpha B_1 + \beta B_2)] = \alpha [A \mid B_1] + \beta [A \mid B_2] 
\]

**(iii) Positivité définie** :\(\forall\) \(A \in M_{n,p}(\mathbb{R}) \)

\[
[A \mid A] \geq 0 \quad \text{et} \quad [A \mid A] = 0 \text{ si et seulement si } A = 0.
\]

Vérifions ces propriétés :

(i) Soit \(A, B \in M_{n,p}(\mathbb{R}) \)

\begin{align*}
[A \mid B] &= \text{tr}(\tilde{A}'\tilde{B}) \\
&= \text{tr}((W^{\frac{1}{2}} A C^{\frac{1}{2}})' W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B C^{\frac{1}{2}})
\end{align*}

Or, par la propriété de la trace, \(\text{tr}(AB) = \text{tr}(BA)\) et l'invariance par transposition des matrices de poids \(\forall k \in \mathbb{R}, \quad {W^k}' = W^k\) et \({C^k}' = C^k\), on a alors:

\begin{align*}
\text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B C^{\frac{1}{2}}) &= \text{tr}(C^{\frac{1}{2}} B' W^{\frac{1}{2}} W^{\frac{1}{2}} A C^{\frac{1}{2}}) \\
&= [B \mid A]
\end{align*}

(ii) Soit \(A, B_1, B_2 \in M_{n,p}(\mathbb{R}) \) et \(\alpha, \beta \in \mathbb{R}\)

\begin{align*}
[A \mid (\alpha B_1 + \beta B_2)] &= \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} (\alpha B_1 + \beta B_2) C^{\frac{1}{2}}) \\
&= \alpha \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B_1 C^{\frac{1}{2}}) + \beta \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B_2 C^{\frac{1}{2}}) \\
&= \alpha \text{tr}(\tilde{A}'\tilde{B_1}) + \beta \text{tr}(\tilde{A}'\tilde{B_2})\\
&= \alpha [A \mid B_1] + \beta [A \mid B_2] 
\end{align*}

La linéarité de la trace assure que cette propriété est respectée.

(iii) Soit \(A \in M_{n,p}(\mathbb{R}) \)

\[
[A \mid A]=\text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} A C^{\frac{1}{2}}) \geq 0
\]
On suppose que :
\begin{align*}
[A \mid A] &= 0\\
\iff tr(\tilde{A}'\tilde{A}) &=0\\
\iff \tilde{A}'\tilde{A} &=0\\
\iff \tilde{A} &=0\\
\iff A &=0 
\end{align*} 
(car W et C ne sont pas nulle)

Par conséquent, la quantité \([A \mid B] = \text{tr}(\tilde{A}'\tilde{B})\) est un produit scalaire, car elle satisfait toutes les propriétés requises.

c) Écrivons le produit scalaire précédent sous forme de double somme. On note \(\tilde{B} = (\tilde{b}_{ij})\), d'où \([ \tilde{A} \tilde{B} ] = \sum_{k=1}^{n} \tilde{a}_{ik} \tilde{b}_{kj}\) et \([ \tilde{A}' \tilde{B} ] = \sum_{k=1}^{n} \tilde{a}_{ki} \tilde{b}_{kj}\). On en déduit alors:
\begin{align*}
[A \mid A] &= tr(\tilde{A}'\tilde{B})\\
 &= \sum_{l=1}^{p} \sum_{k=1}^{n} \tilde{a}_{kl} \tilde{b}_{kl}
\end{align*}

d) Nous pouvons observer le programme du précédent  produit scalaire ci-dessous, ainsi que la norme associée à ce produit scalaire.

La fonction calculant le produit scalaire de Frobénius 
```{r}
prd_scalaire = function(A,B){
  n = length(A[,1]); p = length(A[1,])
  # Les matrices de ponderation
  W = (1/n)*diag(n); C = (1/p)*diag(p)
  
  # Calcul des matrices A_tild et B_tild
  A_tild = sqrt(W) %*% A %*% sqrt(C)
  B_tild = sqrt(W) %*% B %*% sqrt(C)
  
  # Produit scalaire
  ps = sum(diag(t(A_tild)%*%B_tild))
  
  return(ps)
 }
```

Exemple d'application sur le produit scalaire de Frobénius
```{r}
# On initialise des matrices au hasard
A  = matrix(-16:18, nrow =7)
B = matrix(1:35, nrow =7)

# On applique la fonction prd_scalaire
resultat = prd_scalaire(A,B)

# Affichage du résultat
print(resultat)
```

La fonction calculant la norme d'une matrice A associée au produit scalaire de Frobénius 
```{r}
norme = function(A){
  return(sqrt(prd_scalaire(A,A)))
}
```

Exemple d'application sur la norme
```{r}
# On utilise les matrices précédente afin de calculer leurs normes
rn1 = norme(A)
rn2 = norme(B)

# Affichage du résultat
rn1; rn2
```

### 1.2 Coefficient RV

2. On définit le coefficient RV d'Escoufier entre deux matrices \( A \) et \( B \) de taille \( (n , p) \) par :
\[ R(A ,B) = \frac{[ A \mid B]}{\left[\left| A \right|\right] \left[\left| B \right|\right] } \]

a) En terme géométrique le coefficient RV est le cosinus de A et B.

b) Ci-dessous nous trouverons le programme calculant le coefficient RV ainsi qu'une fonction donnant la matrice des coeffients RV en T tableaux \(X_{(n,p)} \)

La fonction calculant le coefficient RV d'Escoufier                  
```{r}
coef_RV = function(T_tableaux) {
  t = length(T_tableaux)
  mat_rv = matrix(rep(NA, t * t), nrow = t, ncol = t)
  
  for (i in seq_along(T_tableaux)) {
    for (j in seq_along(T_tableaux)) {
      # Stocker le résultat dans une matrice
      prd_sclr = prd_scalaire(T_tableaux[[i]], T_tableaux[[j]])
      norm_i = norme(T_tableaux[[i]])
      norm_j = norme(T_tableaux[[j]])
      mat_rv[i, j] = prd_sclr / (norm_j * norm_i)
    }
  }
  
  return(mat_rv)
}

```


Exemple d'application sur le coefficient RV d'Escoufier T tableaux X_t(n , p) 
```{r}
X1 <- matrix(-12:22, nrow = 7)
X2 <- matrix(-20:14, nrow = 7)
X3 <- matrix(-16:18, nrow = 7)
X4 <- matrix(1:35, nrow = 7)
X5 <- matrix(-8:26, nrow = 7)
X6 <- matrix(-10:24, nrow = 7)
X7 <- matrix(-16:18, nrow = 7)
X8 <- matrix(-4:30, nrow = 7)
X9 <- matrix(-2:32, nrow = 7)
X10 <- matrix(-1:31, nrow = 7)

# Noms de lignes
noms_lignes <- c("Ligne1", "Ligne2", "Ligne3", "Ligne4", "Ligne5", "Ligne6", "Ligne7")

# Ajouter les noms de lignes aux matrices
rownames(X1) <- noms_lignes
rownames(X2) <- noms_lignes
rownames(X3) <- noms_lignes
rownames(X4) <- noms_lignes
rownames(X5) <- noms_lignes
rownames(X6) <- noms_lignes
rownames(X7) <- noms_lignes
rownames(X8) <- noms_lignes
rownames(X9) <- noms_lignes
rownames(X10) <- noms_lignes


n_tableaux = list(X1, X2, X3, X4, X5, X6, X7, X8,X9,X10)
resultats_n = coef_RV(n_tableaux)
print(data.frame(resultats_n))
```

## 2. Programme de STATIS1

### 2.1. Programme


a) L'expression \(\left[\left| \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right|\right]^2\)
représente l'inertie dans le contexte de l'analyse factorielle. L'inertie est également appelée somme des carrés des corrélations ou variance totale. Dans le cadre de l'analyse factorielle, cette quantité mesure la dispersion totale des données dans l'espace factoriel.

L'objectif du problème d'optimisation associé est de maximiser cette inertie sous la contrainte que la norme du vecteur $u$ est égale à 1. Cela revient à trouver la direction dans laquelle la dispersion des données est maximale.

b) Résolvons le programme ci-dessus:

\[
\max_{\left[\left| u \right|\right]^2 = 1} \left[\left| \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right|\right]^2 = \max_{\left[\left| u \right|\right]^2 = 1} \mathrm{tr}\left( C \left( \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right)' W \left( \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right) \right) 
\]

\[
\iff \max_{\left[\left| u \right|\right]^2 = 1} \left[\left| \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right|\right]^2= \sum_{t=1}^{T} \sum_{\tau =1}^{T} \frac{1}{\left[\left| X_t \right|\right] \left[\left| X_{\tau} \right|\right]} u_t u_{\tau} \mathrm{tr}\left( CX_{\tau}'WX_t \right) 
\]

1. \(X_t\): \(n \times p\) (la matrice \(X_t\) a des dimensions \(n \times p\)).
2. \(\frac{u_t}{\left[\left| X_t \right|\right]} X_t\): \(n \times p\) (chaque colonne de \(X_t\) est pondérée par \(\frac{u_t}{\left[\left| X_t \right|\right]}\)).
3. \(\sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t\): \(n \times p\) (somme des termes précédents sur \(t\)).
4. \(\left(\sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t\right)'\): \(p \times n\) (transposée de la matrice résultante).
5. \(\left[\left| \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right|\right]^2\): \(1 \times 1\) (la norme euclidienne au carré est un scalaire).

Le langrangien associée au problème d'optimisation ci-dessus s'écrit:
\[L(u,\lambda) = \sum_{t=1}^{T}\sum_{\tau=1}^{T}\frac{u_t u_{\tau}}{\left[\left| X_t \right|\right]\left[\left| X_{\tau} \right|\right]} tr(\tilde{X_t'\tilde{X_{\tau}}}) - \lambda( \|u\|^2-1) \]
On a alors :

\[
\begin{cases}
  \frac{\partial L}{\partial u}(u,\lambda) = 2\sum_{t=1}^{T}\sum_{\tau=1}^{T}\frac{ u_{\tau}}{\left[\left| X_t \right|\right]\left[\left| X_{\tau} \right|\right]} tr(\tilde{X_t'\tilde{X_{\tau}}}) - 2\lambda u \\
  \frac{\partial L}{\partial \lambda}(u,\lambda) = \|u\|^2 - 1
\end{cases}
\]
\[\iff (S)
\begin{cases}
  \sum_{t=1}^{T}\sum_{\tau=1}^{T}\frac{tr(\tilde{X_t'\tilde{X_{\tau}}}) }{\left[\left| X_t \right|\right]\left[\left| X_{\tau} \right|\right]}u_{\tau}  = \lambda u \quad (*) \\
  \|u\|^2 = 1 \quad (**)
\end{cases}
\]

D'une part, en multipliant l'équation \((*) \quad par \quad u'\) on obtient grâce à l'équation \((**)\) le résultat suivant:
\[\sum_{t=1}^{T}\sum_{\tau=1}^{T}\frac{u_t tr(\tilde{X_t'\tilde{X_{\tau}}}) u_{\tau}}{\left[\left| X_t \right|\right]\left[\left| X_{\tau} \right|\right]} = \lambda \]

D'autre part, en multipliant par \(Z := [X_1|...|X_T]\) l'équation $(*)$ on obtient le résultat suivant:
\[\sum_{t=1}^{T}\sum_{\tau=1}^{T}\frac{ tr(\tilde{X_t'\tilde{X_{\tau}}}) }{\left[\left| X_t \right|\right]\left[\left| X_{\tau}  \right|\right]}X_tu_{\tau} = \lambda \sum_{t =1}^{T}X_t u_t \]

D'après, $(S)$ on en déduit que les vecteurs u solution du premier ordre sont les vecteurs propres de la de la matrice \(\Gamma = \sum_{t=1}^{T}\sum_{\tau=1}^{T}\frac{\tilde{X_t'}\tilde{X_{\tau}} }{\left[\left| X_t \right|\right]\left[\left| X_{\tau} \right|\right]} \) des coefficients RV d'Escoufier entre T tableaux \(X_t(n , p)\).

c) Nous allons écrivez le programme R fournissant les vecteurs u solutions des équations du premier
ordre. 


Fonction donnant les vecteurs u solution et valeurs propres

```{r}
vecval_prop = function(T_tableau) {
  matrice = coef_RV(T_tableau)# on calcule la matrice contenant les coefs d'Escoufier
  
  resultat_propre = eigen(matrice)# list ayant les valeurs et vecteurs propres
  val_prop = resultat_propre$values #valeurs propres
  vec_prop = resultat_propre$vectors #vecteurs propres associées
  
  return(list(val_prop = val_prop, vec_prop = vec_prop))
}
```


exemple d'application de la fonction vecval_prop
```{r}
vecval = vecval_prop(n_tableaux)

val_prop <- vecval$val_prop
vec_prop <- vecval$vec_prop

# Affichage des valeurs propres et des vecteurs propres
print("Valeurs propres :")
print(val_prop)
print("Vecteurs propres :")
print(data.frame(vec_prop))
```
Montrons à présent que les vecteurs u obtenus forment une base I-orthonormée.

La fonction **"vecval_prop()"** ci-dessus nous donne les vecteurs propres de la matrice des coefficients RV \(\Gamma\) associée  aux valeurs propres \(\lambda\). Ensuite, nous utilisons la fonction **"verifier_orthogonalite()"** ci-dessous afin de calculé le produit scalire euclidienne entre les vecteurs propres. Si les vecteurs sont orthogonaux, la fonction nous renverra **"Les vecteurs propres sont orthogonaux entre eux"** dans le cas contraire elle affichera **"Les vecteurs propres ne sont pas orthogonaux entre eux"**.



```{r}
# Fonction pour vérifier l'orthogonalité des vecteurs propres
verifier_orthogonalite <- function(vec_prop) {
  n <- nrow(vec_prop) # Nombre de vecteurs propres
  orthogonale <- TRUE
  
  # Vérifier l'orthogonalité pour chaque paire de vecteurs propres
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      produit_scalaire <- sum(vec_prop[, i] * vec_prop[, j]) # Produit scalaire
      if (abs(produit_scalaire) > 1e-14) { # Vérifier si le produit scalaire est proche de zéro
        orthogonale <- FALSE
        break
      }
    }
    if (!orthogonale) {
      break
    }
  }
  
  return(orthogonale)
}
```


```{r}
# fonction pour vérifier si les vecteurs propres sont bien de norme 1
verifier_norme = function(vec_prop){
  n = length(vec_prop[,1]) 
  v = rep(1,n)
  int = rep(NA,n)
  
  for (i in 1:n) {
      p = t(vec_prop[,i]) %*% vec_prop[,i]
      int[i] = p
  }
  
  if(sum(int) == sum(v)){
    print("les vecteurs sont bien de norme 1")
  }else{
    print("il existe un vecteurs qui n'est pas de norme 1")
 }
}

```


Utilisation de les fonctions pour vérifier si les vecteurs propres forment une base orthonormée:
```{r}
orthogonale = verifier_orthogonalite(vec_prop)

# Affichage du résultat
if (orthogonale) {
  print("Les vecteurs propres sont orthogonaux entre eux.")
} else {
  print("Les vecteurs propres ne sont pas orthogonaux entre eux.")
}

verifier_norme(vec_prop)
```




### 2.2. Équivalence avec l'ACP d'un tableau juxtaposé "dépliant" le tableau cubique

a) 

Dans le contexte de l'ACP des tableaux juxtaposés, les "variables" sont les différents tableaux $\mathbf{X_t}$, et les "individus" sont les observations à chaque période de temps.

Dans notre cas, chaque $\mathbf{X_t}$ représente un tableau de données à une période de temps spécifique, où les lignes représentent les individus et les colonnes représentent les variables. Par conséquent, chaque colonne de $\mathbf{X_t}$ représente une variable différente observée à cette période.

Ainsi, en considérant les tableaux $\mathbf{X_t}$ comme les variables, la matrice $\mathbf{A}$ est calculée en utilisant les produits scalaires entre les différentes combinaisons de tableaux $\mathbf{X_t}$. Les valeurs propres et les vecteurs propres de cette matrice $\mathbf{A}$ correspondent alors aux directions dans lesquelles la dispersion des données est maximale dans l'espace des tableaux juxtaposés.



b) Pour déduire que les composantes principales \( F^1, \ldots, F^k, \ldots \) sont orthogonales au sens du produit scalaire convenable, nous devons considérer la façon dont ces composantes sont construites à partir des vecteurs propres obtenus.

Lorsque nous obtenons les vecteurs propres \( u_k \) de la matrice des coefficients RV, nous avons montré précédemment qu'ils forment une base orthogonale au sens du produit scalaire euclidien. Cela signifie que ces vecteurs propres sont mutuellement orthogonaux.

Ensuite, les composantes principales \( F^k \) sont obtenues en projetant les tableaux \( X_t \) sur les vecteurs propres \( u_k \). Comme chaque vecteur propre \( u_k \) est orthogonal aux autres vecteurs propres, les projections des tableaux sur ces vecteurs propres le seront également. Par conséquent, les composantes principales \( F^k \) seront orthogonales les unes aux autres.


c) Dépliage du tableau cubique en un tableau juxtaposé :
```{r}
# Création des tableaux X_t dépliés
X1_jux <- matrix(-12:22, nrow = 35)
X2_jux <- matrix(-20:14, nrow = 35)
X3_jux <- matrix(-16:18, nrow = 35)
X4_jux <- matrix(1:35, nrow = 35)
X5_jux <- matrix(-8:26, nrow = 35)
X6_jux <- matrix(-10:24, nrow = 35)
X7_jux <- matrix(-16:18, nrow = 35)
X8_jux <- matrix(-4:30, nrow = 35)
X9_jux <- matrix(-2:32, nrow = 35)
X10_jux <- matrix(-3:31, nrow = 35)

mat_jux <- cbind(X1_jux, X2_jux, X3_jux, X4_jux, X5_jux, X6_jux, X7_jux, X8_jux, X9_jux, X10_jux)


mat_sym <- t(mat_jux_centr) %*% w_jux %*% mat_jux_centr # Matrice de variance-covariance
```

Composantes principales des tableaux dépliés :
```{r}
# On veut donner la fonction des composantes principales correspondant aux vecteurx u (vec_prop)
composantes_principales = function(u, X_t){
  F <- X_t %*% u # calcul de la matrice des composantes principales
  
  return(F)
}
print(composantes_principales(vec_prop, mat_jux)) # Affichage des composantes principales

# On veut maintenant programmer la fonction calculant la kième composante F_k ainsi que la composante normée f_k correspondante
composante_k = function(u, X_t, k){
  
  F_k = X_t %*% u # calcul des composantes principales

  
  f_k = F_k / sqrt(sum(F_k^2)) # calcul de la kième composante principale normée
  
  return(list(F_k = F_k[,k], f_k = f_k[,k]))
}
print(composante_k(vec_prop, mat_jux, 1)) # Affichage de la première composante principale et de sa version normée
print(composante_k(vec_prop, mat_jux, 2)) # Affichage de la deuxième composante principale et de sa version normée

# Fonction donnant la représentation graphique des individus en plan principal (k,l)
representation_graphique = function(vec_prop, X_t, k, l){
  F_k = composante_k(vec_prop, X_t, k)$F_k # Récupération de la kième composante principale
  F_l = composante_k(vec_prop, X_t, l)$F_k # Récupération de la lième composante principale
  
  plot(F_k, F_l, xlab = paste("F", k, sep = ""), ylab = paste("F", l, sep = ""), main = paste("Représentation des individus en plan principal (", k, ",", l, ")", sep = ""))
  abline(h = 0, col = "gray")
  abline(v = 0, col = "gray")
}
representation_graphique(vec_prop, mat_jux, 1, 2) # Affichage de la représentation graphique des individus en plan principal (1,2)

```
d) Cosinus entre les tableaux et les composantes principales :

```{r}
# Calcul des cosinus entre Xt (resultats_n) et chaque composante principale F_k
cosinus = function(X_t, F_k){
  for (k in 1:ncol(F_k)){
    cos = sum(X_t * F_k[,k]) / (sqrt(sum(X_t^2)) * sqrt(sum(F_k^2))) # Calcul du cosinus entre Xt et la kième composante F_k
    print(cos)
  }
}
cosinus(resultats_n, composantes_principales(vec_prop, resultats_n)) # Affichage du produit scalaire entre Xt et la ième composante principale)

# Fonction donnant la représentation graphique des variables en plan principal (k,l)
representation_graphique_variables = function(vec_prop, X_t, k, l){
  F_k = composante_k(vec_prop, X_t, k)$F_k # Récupération de la kième composante principale
  F_l = composante_k(vec_prop, X_t, l)$F_k # Récupération de la lième composante principale
  
  plot(F_k, F_l, xlab = paste("F", k, sep = ""), ylab = paste("F", l, sep = ""), main = paste("Représentation des variables en plan principal (", k, ",", l, ")", sep = ""))
  abline(h = 0, col = "gray")
  abline(v = 0, col = "gray")
}
representation_graphique_variables(vec_prop, resultats_n, 1, 2) # Affichage de la représentation graphique des variables en plan principal (1,2)

```

### 2.3. ACP et d'autres dépliages du tableau cubique 
a) Il y a deux autres dépliages possibles du tableau cubique selon ce que l'on définit comme individus et variables. 

b) Quelle(s) ACP des tableaux dépliés pouvez-vous envisager? Pour chacune: quels sont ses
"individus" et ses "variables"? Quel centrage paraît opportun? Quelle réduction? Quels
éléments sont utiles à projeter en supplémentaire?
Les ACP à envisager dépendent du jeu de données que l'on souhaite étudier. Ici nous avons créé notre jeu de données sans réelle signification. De même les indidus et variables restent à déterminer. Ce qui est sûr c'est que les individus regrouperont les tableaux dépliés. Les éléments à projeter en supplémentaire dépendent de ce que l'on souhaite étudier.

c) Quels sont les avantages et les inconvénients de chaque ACP d'un tableau "déplié"?
Finalement, le "dépliage" opéré par STATIS1 était-il le plus opportun?
Le principal inconvénient est qu'on perde une grosse partie de l'information qui sera noyée dans le tableau déplié. Cependant, cela permet de mieux visualiser les données et de les traiter plus facilement. 

## II - Situation 2


```{r}

```