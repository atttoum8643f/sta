---
title: "Projet STATIS: Réponse feuille de route"
output: pdf_document
---
## I-Préliminaires pour la situation 1

### 1.1 Produit scalaire 

La matrice diagonale des poids des \( n \) individus est \( W \) (par défaut, \( W = \frac{1}{n} I_n \)).
On considérera également une matrice diagonale des poids des \( p \) colonnes : \( C = \frac{1}{p} I_p \) par défaut.

1. Le produit scalaire entre deux matrices \( A \) et \( B \) de taille \( (n, p) \) est :

\[ [A|B] = \text{tr}(CA'WB) \]

La norme d'une matrice \( A \) correspondant à ce produit scalaire sera notée \( \left[\left| A \right|\right] \).

a) Ecrivons le produit scalaire sous forme \(tr(\tilde{A}'\tilde{B})\) (produit scalaire de Frobénius) en explicitant la transformation \(Z \text{ vers } \tilde{Z}\), \( \forall Z \) \( (n, p) \).

Soit \(A,B \in M_{n,p}(\mathbb{R}) \)

\begin{align*}
[A \mid B] &= \text{tr}(CA'WB) \\
&= \text{tr}(C^{\frac{1}{2}} A'W^{\frac{1}{2}} W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}((W^{\frac{1}{2}} A C^{\frac{1}{2}})' W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}(\tilde{A}'\tilde{B})
\end{align*}

où \(\forall (n,p)\) \(\tilde{Z} =  W^{\frac{1}{2}} Z C^{\frac{1}{2}}\)

b) Pour montrer que \([A \mid B] = \text{tr}(\tilde{A}'\tilde{B})\) est un produit scalaire, nous devons démontrer les propriétés suivantes : 

**(i) Symétrie** : \(\forall\) \(A,B \in M_{n,p}(\mathbb{R}) \)

\[
[A \mid B] = [B \mid A]
\]

**(ii) Linéarité** :\(\forall\) \(A, B_1, B_2 \in M_{n,p}(\mathbb{R}) \) et \(\alpha, \beta \in \mathbb{R}\)

\[
[A \mid (\alpha B_1 + \beta B_2)] = \alpha [A \mid B_1] + \beta [A \mid B_2] 
\]

**(iii) Positivité définie** :\(\forall\) \(A \in M_{n,p}(\mathbb{R}) \)

\[
[A \mid A] \geq 0 \quad \text{et} \quad [A \mid A] = 0 \iff A = 0.
\]

Vérifions ces propriétés :

(i) Soit \(A, B \in M_{n,p}(\mathbb{R}) \)

\begin{align*}
[A \mid B] &= \text{tr}(\tilde{A}'\tilde{B}) \\
&= \text{tr}((W^{\frac{1}{2}} A C^{\frac{1}{2}})' W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B C^{\frac{1}{2}})
\end{align*}

Or, par la propriété de la trace, \(\text{tr}(AB) = \text{tr}(BA)\) et l'invariance par transposition des matrices de poids \(\forall k \in \mathbb{R}, \quad {W^k}' = W^k\) et \({C^k}' = C^k\), on a alors:

\begin{align*}
\text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B C^{\frac{1}{2}}) &= \text{tr}(C^{\frac{1}{2}} B' W^{\frac{1}{2}} W^{\frac{1}{2}} A C^{\frac{1}{2}}) \\
&= [B \mid A]
\end{align*}

(ii) Soit \(A, B_1, B_2 \in M_{n,p}(\mathbb{R}) \) et \(\alpha, \beta \in \mathbb{R}\)

\begin{align*}
[A \mid (\alpha B_1 + \beta B_2)] &= \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} (\alpha B_1 + \beta B_2) C^{\frac{1}{2}}) \\
&= \alpha \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B_1 C^{\frac{1}{2}}) + \beta \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} B_2 C^{\frac{1}{2}}) \\
&= \alpha \text{tr}(\tilde{A}'\tilde{B_1}) + \beta \text{tr}(\tilde{A}'\tilde{B_2})\\
&= \alpha [A \mid B_1] + \beta [A \mid B_2] 
\end{align*}

La linéarité de la trace assure que cette propriété est respectée.

(iii) Soit \(A \in M_{n,p}(\mathbb{R}) \), on note \((\tilde{a}_{ij})\) le terme générique de la matrice \(\tilde{A}\). On pose \(\Delta = \tilde{A}'\tilde{A}\) et on note \((\delta_{ij})\) le terme générique de la matrice \(\Delta\).

\begin{align*}
[A \mid A] &= \text{tr}(C^{\frac{1}{2}} A' W^{\frac{1}{2}} W^{\frac{1}{2}} A C^{\frac{1}{2}})\\
&= tr(\tilde{A}'\tilde{A})\\
&= tr(\Delta)\\
&= \sum_{i=1}^{p} \delta_{ii}
\end{align*}

Or, \(\delta_{ij} = \sum_{k=1}^{p} \tilde{a}_{ki} \tilde{a}_{kj}\) donc:

\begin{align*}
tr(\tilde{A}'\tilde{A}) &= tr(\Delta)\\
&= \sum_{i=1}^{p} \sum_{k=1}^{n} \tilde{a}_{ki} \tilde{a}_{ki}\\
&= \sum_{i=1}^{p} \sum_{k=1}^{n} \tilde{a}_{ki}^2 \geq 0
\end{align*}

On suppose que \([A \mid A] = 0\) montrons que \(A = 0_{\mathbb{R}^{n \times p}}\) :

\begin{align*}
[A \mid A] &= 0\\
\Rightarrow tr(\tilde{A}'\tilde{A}) &= 0\\
\Rightarrow tr(\Delta) &= 0\\
\Rightarrow \sum_{i=1}^{p} \delta_{ii} &= 0\\
\Rightarrow \sum_{i=1}^{p} \sum_{k=1}^{n} \tilde{a}_{ki}^2 &= 0
\end{align*}

Cela implique que 

\[\forall (i,k) \in [|1,p|] \times [|1,n|], \quad \tilde{a}_{ki}^2 = 0 \]

donc 

\[\forall (i,k) \in [|1,p|] \times [|1,n|], \quad \tilde{a}_{ki} = 0 \] 

donc

\[\tilde{A} = W^{\frac{1}{2}} A C^{\frac{1}{2}} = 0_{\mathbb{R}^{n \times p}} \] 

On en déduit que \(A = 0_{\mathbb{R}^{n \times p}}\) car \(W\) et \(C\) sont régulières donc inversibles.

Par conséquent, la quantité \([A \mid B] = \text{tr}(\tilde{A}'\tilde{B})\) est un produit scalaire, car elle satisfait toutes les propriétés requises.

c) Écrivons le produit scalaire précédent sous forme de double somme. Soit \(A,B \in M_{n,p}(\mathbb{R}) \) on note \((\tilde{a}_{ij})\) respectivement \((\tilde{b}_{ij})\) le terme générique de la matrice \(\tilde{A}\) respectivement \(\tilde{B}\). On pose \(\Gamma = \tilde{A}'\tilde{B}\) et on note \((\gamma_{ij})\) le terme générique de la matrice de  \(\Gamma\). Ainsi:

\begin{align*}
[A \mid B] &= tr(\tilde{A}'\tilde{B})\\
&= tr(\Gamma)\\
&= \sum_{l=1}^{p} \gamma_{ll}\\
 &= \sum_{l=1}^{p} \sum_{k=1}^{n} \tilde{a}_{kl} \tilde{b}_{kl}
\end{align*}

d) Nous pouvons observer le programme du précédent  produit scalaire ci-dessous, ainsi que la norme associée à ce produit scalaire.

La fonction calculant le produit scalaire de Frobénius
```{r,include=FALSE}
library(multiblock)
library(ggplot2)
library(ade4)
```

```{r}
prd_scalaire = function(A,B){
  # les dimension des matrices
  na = length(A[,1]); nb = length(B[,1])
  pa = length(A[1,]); pb = length(B[1,])
  
  # Les matrices de ponderation
  Wa = (1/na)*diag(na); Ca = (1/pa)*diag(pa)
  Wb = (1/nb)*diag(na); Cb = (1/pb)*diag(pb)
  
  # Calcul des matrices A_tild et B_tild
  A_tild = sqrt(Wa) %*% A %*% sqrt(Ca)
  B_tild = sqrt(Wb) %*% B %*% sqrt(Cb)
  
  # Produit scalaire
  ps = sum(diag(t(A_tild)%*%B_tild))
  
  return(ps)
 }
```

Exemple d'application sur le produit scalaire de Frobénius
```{r}
# On initialise des matrices au hasard
A  = matrix(-16:18, nrow =7)
B = matrix(1:35, nrow =7)

# On applique la fonction prd_scalaire
resultat = prd_scalaire(A,B)

# Affichage du résultat
print(resultat)
```

La fonction calculant la norme d'une matrice A associée au produit scalaire de Frobénius 
```{r}
norme = function(A){
  return(sqrt(prd_scalaire(A,A)))
}
```

Exemple d'application sur la norme
```{r}
# On utilise les matrices précédente afin de calculer leurs normes
rn1 = norme(A)
rn2 = norme(B)

# Affichage du résultat
rn1; rn2
```

### 1.2 Coefficient RV

On définit le coefficient RV d'Escoufier entre deux matrices \( A \) et \( B \) de taille \( (n , p) \) par :
\[ R(A ,B) = \frac{[ A \mid B]}{\left[\left| A \right|\right] \left[\left| B \right|\right] } \]

a) En termes géométriques le coefficient RV et le cosinus de A et B.

b) Ci-dessous nous trouverons le programme calculant le coefficient RV ainsi qu'une fonction donnant la matrice des coeffients RV en T tableaux \(X_{(n,p)} \).

La fonction calculant le coefficient RV d'Escoufier:                  
```{r}
coef_RV = function(T_tableaux) {
  t = length(T_tableaux)
  mat_rv = matrix(rep(NA, t * t), nrow = t, ncol = t)
  
  for (i in seq_along(T_tableaux)) {
    for (j in seq_along(T_tableaux)) {
      # Stocker le résultat dans une matrice
      prd_sclr = prd_scalaire(T_tableaux[[i]], T_tableaux[[j]])
      norm_i = norme(T_tableaux[[i]])
      norm_j = norme(T_tableaux[[j]])
      mat_rv[i, j] = prd_sclr / (norm_j * norm_i)
    }
  }
  
  return(mat_rv)
}

```


Exemple d'application sur le coefficient RV d'Escoufier T tableaux X_t(n , p) 
```{r}
# Tableau à trois dimension
data(simulated)

A = simulated$A
B = simulated$B
C = simulated$C
D = simulated$D

n_tableaux = list(A,B,C,D)

resultats_n = coef_RV(n_tableaux)
print(data.frame(resultats_n))
```

## 2. Programme de STATIS1

### 2.1. Programme


a) L'expression \(\left[\left| \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right|\right]^2\)

représente l'inertie dans le contexte de l'analyse factorielle. L'inertie est également appelée somme des carrés des corrélations ou variance totale. Dans le cadre de l'analyse factorielle, cette quantité mesure la dispersion totale des données dans l'espace factoriel (l'espace ou les individus se trouvent).

L'objectif du problème d'optimisation associé est de maximiser cette inertie sous la contrainte que la norme du vecteur $u$ est égale à 1. Cela revient à trouver la direction dans laquelle la dispersion des données est maximale. L'inertie d'un vecteur u par rapport à une matrice X est définie comme la norme au carré de la projection de X sur u. Plus précisément, \(\sum_{t=1}^{T}(u^TX_t)^2\) représente la somme des inerties  de tous les vecteurs \(X_t\) projetés sur u.

b) Résolvons le programme ci-dessus:

\begin{align*}
\max_{\left[\left| u \right|\right]^2 = 1} \left[\left| \sum_{t=1}^{T} \frac{X_t}{\left[\left| X_t \right|\right]} u_t \right|\right]^2 &= \max_{\left[\left| u \right|\right]^2 = 1} \sum_{t=1}^{T} \sum_{\tau =1}^{T} u_{\tau} \frac{\mathrm{tr}\left( CX_{\tau}'WX_t \right)}{\left[\left| X_t \right|\right] \left[\left| X_{\tau} \right|\right]} u_t  \\
&= \max_{\left[\left| u \right|\right]^2 = 1} \sum_{t=1}^{T} \sum_{\tau =1}^{T} u_{\tau} \frac{[X_{\tau} \mid X_t]}{\left[\left| X_t \right|\right] \left[\left| X_{\tau} \right|\right]} u_t \\
&= \max_{\left[\left| u \right|\right]^2 = 1} u'R u
\end{align*}


1. \(X_t\): \(n \times p\) (la matrice \(X_t\) a des dimensions \(n \times p\)).
2. \(\frac{u_t}{\left[\left| X_t \right|\right]} X_t\): \(n \times p\) (chaque colonne de \(X_t\) est pondérée par \(\frac{u_t}{\left[\left| X_t \right|\right]}\)).
3. \(\sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t\): \(n \times p\) (somme des termes précédents sur \(t\)).
4. \(\left(\sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t\right)'\): \(p \times n\) (transposée de la matrice résultante).
5. \(\left[\left| \sum_{t=1}^{T} \frac{u_t}{\left[\left| X_t \right|\right]} X_t \right|\right]^2\): \(1 \times 1\) (la norme euclidienne au carré est un scalaire).
6. \(R\) est la matrice des coefficients RV

Le langrangien associée au problème d'optimisation ci-dessus s'écrit:

\[L(u,\lambda) = u'R u - \lambda (\|u\|^2-1) \]

On a alors :

\[
\begin{cases}
  \frac{\partial L}{\partial u}(u,\lambda) = 2Ru - 2\lambda u \\
  \frac{\partial L}{\partial \lambda}(u,\lambda) = \|u\|^2 - 1
\end{cases}
\]
\[\iff (S)
\begin{cases}
  Ru  = \lambda u \quad (*) \\
  \|u\|^2 = 1 \quad (**)
\end{cases}
\]

En multipliant l'équation \((*) \quad par \quad u'\) on obtient grâce à l'équation \((**)\) le résultat suivant:

\[u'Ru = \lambda \]

D'après, $(S)$ on en déduit que les vecteurs u solution du premier ordre sont les vecteurs propres de la matrice \(R = \sum_{t=1}^{T}\sum_{\tau=1}^{T}\frac{tr(\tilde{X_t'}\tilde{X_{\tau}}) }{\left[\left| X_t \right|\right]\left[\left| X_{\tau} \right|\right]} \) des coefficients RV d'Escoufier entre T tableaux \(X_t(n , p)\). Or, pour tout vecteur propre $u$ de $R$ (tel que $\|u\| = 1$) de valeur propre $\lambda$, on a
\( u'Ru = \lambda \).

La valeur maximale de $u'Ru$ est obtenue pour les vecteurs propres associés à la plus grande valeur propre de $R$.

c) Nous allons écrivez le programme R fournissant les vecteurs u solutions des équations du premier
ordre. 


Fonction donnant les vecteurs u solution et valeurs propres

```{r}
vecval_prop = function(T_tableau) {
  matrice = coef_RV(T_tableau)# on calcule la matrice contenant les coefs d'Escoufier
  
  resultat_propre = eigen(matrice)# list ayant les valeurs et vecteurs propres
  val_prop = resultat_propre$values #valeurs propres
  vec_prop = resultat_propre$vectors #vecteurs propres associées
  
  return(list(val_prop = val_prop, vec_prop = vec_prop))
}

```


exemple d'application de la fonction vecval_prop
```{r}
vecval = vecval_prop(n_tableaux)

val_prop <- vecval$val_prop
vec_prop <- vecval$vec_prop

# Affichage des valeurs propres et des vecteurs propres
print("Valeurs propres :")
print(val_prop)
print("Vecteurs propres :")
print(data.frame(vec_prop))
```
Montrons à présent que les vecteurs u obtenus forment une base I-orthonormée.

Reprenons la matrice \(R\) des coefficients RV précédente. Soient \(u_i\) et \(u_j\) des vecteurs propres de \(R\) associés aux valeurs propres \(\lambda_i\) et \(\lambda_j\) distinctes. Comme \(u_i\) et \(u_j\) sont des solutions du problème suivant :

\[
\max_{\left[\left| u \right|\right]^2 = 1} u'R u
\]

on a alors \(\|u_i\|^2 = 1\) et \(\|u_j\|^2 = 1\). Il suffit de montrer que \(u_i\) et \(u_j\) sont orthogonaux et ainsi de conclure que les vecteurs \(u\) forment une base I-orthonormée. En d'autres termes, nous allons montrer que \(u_i'u_j = 0\).

Sachant que \(u_i\) et \(u_j\) sont des vecteurs propres de \(R\) associés aux valeurs propres \(\lambda_i\) et \(\lambda_j\), nous avons :

\[
\begin{cases}
  Ru_i  = \lambda_i u_i \\
  Ru_j  = \lambda_j u_j \\
\end{cases}
\]

En prenant le produit scalaire des deux équations, nous obtenons :

\begin{align*}
u_i'Ru_j &= \lambda_j u_i' u_j\\
\iff u_i'R'u_j &= \lambda_j u_i' u_j \quad (\text{car } R \text{ est une matrice symétrique})\\
\iff (Ru_i)'u_j &= \lambda_j u_i' u_j\\
\iff (\lambda_i u_i)'u_j &= \lambda_j u_i' u_j\\
\iff \lambda_i u_i'u_j &= \lambda_j u_i' u_j\\
\iff (\lambda_i - \lambda_j) u_i'u_j &=  0\\
\iff u_i'u_j &=  0 \quad (\text{car } \lambda_i \neq \lambda_j)
\end{align*}

On en déduit donc que les vecteurs \(u\) forment une base I-orthonormée.

La fonction **"vecval_prop()"** ci-dessus nous donne les vecteurs propres \(u_i\) de la matrice des coefficients RV \(R\) associée  aux valeurs propres \(\lambda_i\). 


```{r,include=FALSE}
# Fonction pour vérifier l'orthogonalité des vecteurs propres
verifier_orthogonalite <- function(vec_prop) {
  n <- nrow(vec_prop) # Nombre de vecteurs propres
  orthogonale <- TRUE
  
  # Vérifier l'orthogonalité pour chaque paire de vecteurs propres
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      produit_scalaire <- sum(vec_prop[, i] * vec_prop[, j]) # Produit scalaire
      if (abs(produit_scalaire) > 1e-14) { # Vérifier si le produit scalaire est proche de zéro
        orthogonale <- FALSE
        break
      }
    }
    if (!orthogonale) {
      break
    }
  }
  
  return(orthogonale)
}
```


```{r,include=FALSE}
# fonction pour vérifier si les vecteurs propres sont bien de norme 1
verifier_norme = function(vec_prop){
  n = length(vec_prop[,1]) 
  v = rep(1,n)
  int = rep(NA,n)
  
  for (i in 1:n) {
      p = t(vec_prop[,i]) %*% vec_prop[,i]
      int[i] = p
  }
  
  if(sum(int) == sum(v)){
    print("les vecteurs sont bien de norme 1")
  }else{
    print("il existe un vecteurs qui n'est pas de norme 1")
 }
}

```


```{r,include=FALSE}
orthogonale = verifier_orthogonalite(vec_prop)

# Affichage du résultat
if (orthogonale) {
  print("Les vecteurs propres sont orthogonaux entre eux.")
} else {
  print("Les vecteurs propres ne sont pas orthogonaux entre eux.")
}

verifier_norme(vec_prop)
```


### 2.2. Équivalence avec l'ACP d'un tableau juxtaposé "dépliant" le tableau cubique

a) Dans le contexte de l'ACP des tableaux juxtaposés, les "variables" sont les différents tableaux $\mathbf{X_t}$, et les "individus" sont les observations à chaque période de temps.

Dans notre cas, chaque $\mathbf{X_t}$ représente un tableau de données à une période de temps spécifique, où les lignes représentent les individus et les colonnes représentent les variables. Par conséquent, chaque colonne de $\mathbf{X_t}$ représente une variable différente observée à cette période.


b) Pour déduire que les composantes principales \( F^1, \ldots, F^k, \ldots \) sont orthogonales au sens du produit scalaire convenable, nous devons considérer la façon dont ces composantes sont construites à partir des vecteurs propres obtenus.

Lorsque nous obtenons les vecteurs propres \( u_k \) de la matrice des coefficients RV, nous avons montré précédemment qu'ils forment une base orthogonale au sens du produit scalaire euclidien. Cela signifie que ces vecteurs propres sont mutuellement orthogonaux.

Ensuite, les composantes principales \( F^k \) sont obtenues en projetant les tableaux \( X_t \) sur les vecteurs propres \( u_k \). Comme chaque vecteur propre \( u_k \) est orthogonal aux autres vecteurs propres, les projections des tableaux sur ces vecteurs propres le seront également. Par conséquent, les composantes principales \( F^k \) seront orthogonales les unes aux autres.

c) Dépliage du tableau cubique en un tableau juxtaposé :
```{r}
# Création des tableaux X_t dépliés
A_jux <- matrix(A, ncol = 1)
B_jux <- matrix(B, ncol = 1)
C_jux <- matrix(C, ncol = 1)
D_jux <- matrix(D, ncol = 1)


mat_jux <- cbind(A_jux, B_jux, C_jux, D_jux)

oper_inert = function(X){
  
  p = length(X[1,]) # la dimension des individus
  n = length(X[,1]) # la dimension des variables
  W = diag(n)/n
  M = matrix(0, nrow = p, ncol = p)
  
  for (i in 1:p) {
    M[i,i] = 1/sqrt(t(X[,i])%*%W%*%X[,i])
  }
  mat_d = M %*% t(X) %*% W %*% X %*% M
  return(mat_d)
}

mat_sym <- oper_inert(mat_jux)

  result_propre = eigen(mat_sym)# list ayant les valeurs et vecteurs propres
  val_prop_jux = result_propre$values #valeurs propres
  vec_prop_jux = result_propre$vectors #vecteurs propres associées
```

La fonction **oper_inert()** est définie pour calculer une matrice de covariance normalisée:
Cette fonction prend une matrice **X** en entrée, calcule une matrice de poids **W**, puis calcule une matrice de normalisation **M**. Enfin, elle renvoie la matrice de covariance normalisée **mat_d**.


Composantes principales des tableaux dépliés :
```{r}
# On veut donner la fonction des composantes principales correspondant aux vecteurx u (vec_prop)
composantes_principales = function(u, X_t){
  
  p = length(X_t[1,]) # la dimension des individus
  n = length(X_t[,1]) # la dimension des variables
  W = diag(n)/n
  M = matrix(0, nrow = p, ncol = p)
  
  for (i in 1:p) {
    M[i,i] = 1/sqrt(t(X_t[,i]) %*% W %*% X_t[,i])
  }
  
  F <- X_t %*% M %*% u # calcul de la matrice des composantes principales
  
  return(F)
}
comp_prin = composantes_principales(vec_prop_jux, mat_jux)
print(comp_prin[1:8,1:3]) # Affichage de la première composante principale
```


```{r}
# On veut maintenant programmer la fonction calculant la kième composante F_k ainsi que la 
# composante normée f_k correspondante
composante_k = function(u, X_t, k){
  
   # calcul des composantes principales
  comp_prin = composantes_principales(u, X_t)
  F_k = comp_prin[,k]

  
  f_k = F_k / sqrt(sum(F_k^2)) # calcul de la kième composante principale normée
  
  return(list(F_k = F_k, f_k = f_k))
}

pc1 <- composante_k(vec_prop, mat_jux, 1)$F_k
pc1_nr <- composante_k(vec_prop, mat_jux, 1)$f_k

# Affichage des premières lignes de la première composante principale et de sa version normée
cat("Extrait de la 1ère composante :\n")
print(head(pc1))

cat("\nExtrait de la 1ère composante normée :\n")
print(head(pc1_nr))

```


```{r,fig.height=4.8,fig.width=5.5,fig.align='center'}
# Fonction donnant la représentation graphique des individus en plan principal (k,l)
representation_graph_ind = function(comp_p, val_prop_dep, k, l, 
                                    aff_noms = FALSE, col = "navy") {
  # Créer un data frame avec les composantes principales
  df <- data.frame(F_k = comp_p[, 1], F_l = comp_p[, 2])
  inert_k = val_prop_dep[k] / sum(val_prop_dep) * 100
  inert_l = val_prop_dep[l] / sum(val_prop_dep) * 100
  
  # Récupérer les noms de lignes ou les numéros de ligne
  row_names <- if (is.null(rownames(df))) 1:nrow(df) else rownames(df)
  
  # Trouver les limites des axes x et y pour centrer l'origine
  x_limits <- range(df$F_k)
  y_limits <- range(df$F_l)
  limits <- range(c(x_limits, y_limits))
  
  # Plot les composantes principales dans un plan avec l'origine au centre
  plot(df$F_k, df$F_l, xlab = paste("F", k, " (", round(inert_k, 2), "%)"),
       ylab = paste("F", l, " (", round(inert_l, 2), "%)"),
       main = paste("Représentation des individus dans le plan (", k, ",", l, ")"),
       xlim = limits, ylim = limits)
  
  # Ajouter des lignes en pointillé pour les axes horizontal et vertical
  abline(h = 0, lty = "dotted", lwd = 0.5)
  abline(v = 0, lty = "dotted", lwd = 0.5)
  
  # Afficher les noms de lignes ou les numéros de ligne si demandé
  if (aff_noms) {
    text(df$F_k, df$F_l, labels = row_names, pos = 1, cex = 0.8, col = col)
  }
}

# Exemple d'utilisation sans afficher les noms de lignes ou les numéros de ligne
representation_graph_ind(comp_prin, val_prop_jux, 1, 2)
# Exemple d'utilisation avec l'affichage des noms de lignes ou des numéros de ligne
representation_graph_ind(comp_prin, val_prop_jux, 1, 2, aff_noms = TRUE)

```


```{r,include=FALSE}
geo_ind = function(comp_p, val_prop, k, l, col_points = "black", 
                   col_noms = "navy", aff_noms = FALSE) {
  
  # Créer un data frame avec les composantes principales
  df <- data.frame(PCk = comp_p[, k], PCl = comp_p[, l])
  inert_k = val_prop[k]/sum(val_prop) * 100
  inert_l = val_prop[l]/sum(val_prop) * 100
  
  # Récupérer les noms de lignes ou les numéros de ligne
  row_names <- if (is.null(rownames(df))) 1:nrow(df) else rownames(df)
  
  # Trouver les limites des axes x et y pour centrer l'origine
  x_limits <- range(df$PCk)
  y_limits <- range(df$PCl)
  limits <- range(c(x_limits, y_limits))
  
  # Afficher les composantes principales dans un plan avec l'origine au centre
  plot <- ggplot(df, aes(x = PCk, y = PCl)) +
    geom_point(color = col_points) +
    labs(x = paste("PC", k, " (", round(inert_k, 2), "%)"), y = paste("PC", l, " (", round(inert_l, 2), "%)")) +
    ggtitle(paste("Représentation des individus dans le plan (", k, ",", l, ")")) +
    theme_classic() +
    geom_hline(yintercept = 0, linetype = "dotted", lwd = 0.5) + # Axe horizontal
    geom_vline(xintercept = 0, linetype = "dotted", lwd = 0.5)   # Axe vertical
  
  # Afficher les noms de lignes ou les numéros de ligne si demandé
  if (aff_noms) {
    plot <- plot + geom_text(aes(label = row_names), color = col_noms, size = 3, hjust = -0.2, vjust = -0.2)
  }
  
  plot + xlim(limits) + ylim(limits)
  
}

# Exemple d'utilisation sans afficher les noms de lignes ou les numéros de ligne
geo_ind(comp_prin, val_prop_jux, 1, 2, aff_noms = FALSE)

# Exemple d'utilisation avec l'affichage des noms de lignes ou des numéros de ligne
geo_ind(comp_prin, val_prop_jux, 1, 2, aff_noms = TRUE)

```


d) Cosinus entre les tableaux et les composantes principales :

```{r}
# Calcul des cosinus entre Xt (resultats_n) et chaque composante principale F_k
cosinus = function(X_t, f_k){
  mat_cos = matrix(NA, nrow = ncol(f_k),ncol = ncol(f_k))
  for (k in 1:ncol(f_k)){
    for (j in 1:ncol(f_k)) {
           # Calcul du cosinus entre Xt et la kième composante F_k
           mat_cos[k,j] = sum(X_t[,j] * f_k[,k]) / (sqrt(sum(X_t[,j]^2)) *
                                                      sqrt(sum(f_k^2))) 
           
    }

  }
  rownames(mat_cos) <- paste("v", 1:ncol(mat_cos), sep = "")
  return(mat_cos)
}

# Affichage du produit scalaire entre Xt et la ième composante principale 
cosinus(mat_jux, composantes_principales(vec_prop_jux, mat_jux)) 
```


```{r,fig.height=4.3,fig.width=5.4,fig.align='center'}
representation_graph_var = function(vec_prop, X_t, val_prop, k, l){
  inert_k = val_prop[k]/sum(val_prop) * 100 #l'inertie captée par la comp k
  inert_l = val_prop[l]/sum(val_prop) * 100 #l'inertie captée par la comp k
  
  mat_cos = data.frame(cosinus(X_t, composantes_principales(vec_prop, X_t)))
  F_k = mat_cos[,k] # Récupération de la kième composante principale
  F_l = mat_cos[,l] # Récupération de la lième composante principale

  
  # Création des données pour le cercle unité
  df <- data.frame(
    varabs = cos(seq(0, 2 * pi, length.out = 100)),
    varord = sin(seq(0, 2 * pi, length.out = 100))
  )
  
  # Créer le graphique en utilisant ggplot2
  ggplot(mat_cos, aes(x = F_k, y = F_l)) +
    geom_segment(aes(xend = 0, yend = 0), color = "#445577") +
    geom_text(aes(label = rownames(mat_cos)), hjust = 0, vjust = 0) +
    geom_point(color = "red", size = 1.5) +
    theme_classic() +
    coord_fixed(ratio = 1) +
    geom_hline(yintercept = 0, linetype = "dotted", lwd = 0.5) +
    geom_vline(xintercept = 0, linetype = "dotted", lwd = 0.5) +
    labs(x = paste("F", k, " (", round(inert_k, 2), "%)"),
         y = paste("F", l, " (", round(inert_l, 2), "%)")) +
    ggtitle(paste("Représentation des variables dans le plan (", k, ",", l, ")")) +
    geom_path(data = df, aes(varabs, varord), color = "#222211", linewidth = 0.5) +  # Ajout du cercle unité
    xlim(-1, 1) +
    ylim(-1, 1)
}

# Affichage de la représentation graphique des variables en plan principal (1,2)
representation_graph_var(vec_prop_jux, mat_sym, val_prop_jux, 3, 4)

```

### Illustration d'un ACP d'un tableau juxtaposé dépliant le tableau cubique

Soit \( A \), \( B \) et \( C \) les matrices données comme suit:

\[
\text{A} = \begin{pmatrix}
1 & 0  \\
-1 & 1 \\
2 & 2 
\end{pmatrix}, \quad
\text{B} = \begin{pmatrix}
2 & -1  \\
-1 & 0 \\
0 & 1 
\end{pmatrix}, \quad
\text{C} = \begin{pmatrix}
3 & 1  \\
1 & -2 \\
0 & -3 
\end{pmatrix}
\]

En utilisant le produit scalaire défini par:

\begin{align*}
[A \mid B] &= \text{tr}(CA'WB) \\
&= \text{tr}(C^{\frac{1}{2}} A'W^{\frac{1}{2}} W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}((W^{\frac{1}{2}} A C^{\frac{1}{2}})' W^{\frac{1}{2}} B C^{\frac{1}{2}}) \\
&= \text{tr}(\tilde{A}'\tilde{B})
\end{align*}

avec \(W:= \frac{1}{n}I_n\) la matrice de poids des individus et \(C:= \frac{1}{p}I_p\) la matrice de poids des variables.

Nous allons calculer la matrice des coefficients RV d'Escoufier que l'on notera \(\Gamma\). Ci-dessous la formule permettant de calculer le coefficient RV entre deux matrices \(A\) et \(B\) de taille \(n,p\) ici \(n=3\) et \(p=2\):

\[ R(A ,B) = \frac{[ A \mid B]}{\left[\left| A \right|\right] \left[\left| B \right|\right] } \]

Calculons les coefficients \(R(A ,B)\), \(R(A ,C)\) et \(R(B ,C)\).

Procédons étape par étape:

\[[ A \mid B] = \text{tr}(CA^tWB)= \frac{1}{6}\text{tr}(A^tB)\]
Or, 
\[\frac{1}{6} \begin{pmatrix}
1 & -1 & 2 \\
0 & 1 & 2
\end{pmatrix} \begin{pmatrix}
2 & -1  \\
-1 & 0 \\
0 & 1 
\end{pmatrix} = \frac{1}{6} \begin{pmatrix}
3 & 1  \\
-1 & 2 
\end{pmatrix}\]

donc \( [A \mid B] = \frac{5}{6} \)

Pour \( [A \mid C] \) :

\[
[A \mid C] = \frac{1}{6} \text{tr}(A^T C) \]
Or, 
\[ \frac{1}{6} \begin{pmatrix} 
1 & -1 & 2 \\ 
0 & 1 & 2 
\end{pmatrix} \begin{pmatrix} 
3 & 1 \\ 
1 & -2 \\ 
0 & -3 
\end{pmatrix} = \frac{1}{6} \begin{pmatrix}
2 & -3  \\
1 & -8 
\end{pmatrix}\]

donc \( [A \mid C] = \frac{-6}{6} = -1 \)

Pour \( [B \mid C] \) :

\[
[B \mid C] = \frac{1}{6} \text{tr}(B^T C) \]
Or, 
\[ \frac{1}{6} \begin{pmatrix} 
2 & -1 & 0 \\ 
-1 & 0 & 1 
\end{pmatrix} \begin{pmatrix} 
3 & 1 \\ 
1 & -2 \\ 
0 & -3 
\end{pmatrix} = \frac{1}{6} \begin{pmatrix}
5 & 4  \\
-3 & -4 
\end{pmatrix}
\]

donc \( [B \mid C] = \frac{1}{6} \). En utilisant la norme associée à ce produit scalaire on a:

\(\left[\left| A \right|\right] = \sqrt{\frac{11}{6} } \), \(\left[\left| B \right|\right] = \sqrt{\frac{7}{6}} \) et \(\left[\left| C \right|\right] = \sqrt{\frac{24}{6}} = 2 \).

D'où 

\[R(A ,B) = \frac{ \frac{5}{6} }{ \sqrt{\frac{11}{6}} \sqrt{\frac{7}{6}} } \]

\[R(A ,C) = \frac{ -1 }{ 2 \sqrt{\frac{11}{6}}} \]
et
\[R(B ,C) = \frac{ \frac{1}{6} }{ 2 \sqrt{\frac{7}{6}} } \]

Par symétrie du produit scalaire, la matrice \(\Gamma \) des coefficients RV est donc égale à:

\[\Gamma = \begin{pmatrix}
1.0000000 & 0.56980288 & -0.36927447 \\
0.5698029 & 1.00000000 & 0.07715167 \\
-0.3692745 & 0.07715167 & 1.00000000 \\
\end{pmatrix}\]


Maintenant, considérons le dépliement des matrices juxtaposées :

\[
\text{X} = \begin{pmatrix}
1 & 2 & 3 \\
-1 & -1 & 1 \\
2 & 0 & 0 \\
0 & -1 & 1 \\
1 & 0 & -2 \\
2 & 1 & -3
\end{pmatrix}
\]

où la première variable est la matrice A dépliée, la deuxième variable est la matrice B dépliée et la troisième variable est la matrice C dépliée.

On pose \(M\) la matrice diagonale \(p,p\) (\(ici \quad p = 3, \quad n = 6 )\) dont le coefficient \(M_{ii} = \frac{1}{||X_i||_W} \) avec:

   - \(W:= \frac{1}{n} I_n\)
   - \(X_i\) la ième variable
   - \(\|X_i\|_W = \sqrt{(X_i^tWX_i)}\)

Ici \(M\) sera donc égale à:

\[M = \begin{pmatrix}
\frac{1}{\sqrt{\frac{11}{6}}} & 0 & 0 \\
0 & \frac{1}{\sqrt{\frac{7}{6}}} & 0 \\
0 & 0 & \frac{1}{2} \\
\end{pmatrix}\]

On a alors:

\[\Delta := M X^t W X  M = \Gamma \]

La fonction ci-dessous fait les calculs matriciels puis nous donne la matrice obtenue delta qui est égale à la matrice des coefficients RV calculée précédemment.

```{r}
# Données
X <- matrix(
  c( 1, 2, 3,
    -1,-1, 1,
     2, 0, 0,
     0,-1, 1,
     1, 0,-2,
     2, 1,-3
  ), nrow = 6, byrow = TRUE)

# Ajout des noms de lignes
rownames(X) <- paste0("L", 1:6)

oper_inert = function(X){
  
  p = length(X[1,]) # la dimension des individus
  n = length(X[,1]) # la dimension des variables
  W = diag(n)/n
  M = matrix(0, nrow = p, ncol = p)
  
  for (i in 1:p) {
    M[i,i] = 1/sqrt(t(X[,i])%*%W%*%X[,i])
  }
  return(M %*% t(X) %*% W %*% X %*% M)
}

# Calcul de la matrice
Delta <- oper_inert(X)
print(Delta)

result_propre = eigen(Delta)# list ayant les valeurs et vecteurs propres
valp = result_propre$values #valeurs propres
vecp = result_propre$vectors
```

Finalement, on en déduit qu'il y a équivalence entre les matrices juxtaposées A, B et C avec la matrice X dont les variables sont les matrices A, B et C dépliées.

```{r,fig.height=4.3,fig.width=5.4,fig.align='center'}
comp = composantes_principales(vecp,X)
representation_graph_ind(comp, valp, 1, 3, aff_noms = T)
representation_graph_var(vecp,X,valp,2,3)
```

### 2.3. ACP et d'autres dépliages du tableau cubique 

a) Il y a deux autres dépliages possibles du tableau cubique selon ce que l'on définit comme individus et variables. 

b) Les ACP à envisager dépendent du jeu de données que l'on souhaite étudier. Ici les individus et variables ne sont pas clairement définit car ne porte que des numéros. Ce qui est sûr c'est que les individus regrouperont les tableaux dépliés. Les éléments à projeter en supplémentaire dépendent, encore une fois, de ce que l'on souhaite étudier.

c) Le principal inconvénient est qu'on perde une grosse partie de l'information qui sera noyée dans le tableau déplié. Cependant, cela permet de mieux visualiser les données et de les traiter plus facilement. L'idéal serait de réaliser les trois dépliages possibles et de les comparer pour voir lequel est le plus pertinent. 

## II - Situation 2

### 1. De nouvelles matrices dans un nouvel espace

1.1.a) Calculons \(P_m\) en fonction de \(X_m\) et \(M_m\):
\[P_m = X_m M_m X_m'\]

b) Les matrices \(P_m\) se trouvent dans l'espace \(\mathbf{P} = \mathbf{M}_n(\mathbb{R})\) où \(n\) est le nombre d'individus.

1.2.a) Les poids naturel des lignes de \(P_m\) serait \(W = frac{1}{n}I_n \) même chose ^pour les colonnes.

b) On peut munir \(\mathbf{P}\) du produit scalaire suivant:

\[(P_m,P_k) = tr(P_m W P_k W) \quad (p)\]

c) Les tableaux sont mesurés à des échelles différentes, on se doit donc de normées les tableaux \(P_m\) afin de les ramenées sur la même échelle.

d) Pour mesurer la proximité entre deux matrices \(P_m\) et \(P_k\) on utilise le produit scalaire \((p) \) afin de calculer le coefficient RV entre \(P_m\) et \(P_k\).Si le coefficient est proche de 1 on pourra dire que la matrice \(X_m\) et \(X_k\) sont similaire (correlées fortement). En revanche, si le coefficient RV est proche de -1 on dira que  les matrices \(X_m\) et \(X_k\) sont anticorrelées. Enfin, si le coefficient RV est proche de 0, on dira que \(X_m\) et \(X_k\) sont décorrelées.

### 2.1 Graphiques directs de STATIS

```{r}
# Tableau à trois dimension
data(chickenk)

M = as.matrix(chickenk$Mortality)
rownames(M) = seq(1,351,1)

FS = as.matrix(chickenk$FarmStructure)
rownames(FS) = seq(1,351,1)

OFH = as.matrix(chickenk$OnFarmHistory)
rownames(OFH) = seq(1,351,1)

FC = as.matrix(chickenk$FlockCharacteristics)
rownames(FC) = seq(1,351,1)

CTS = as.matrix(chickenk$CatchingTranspSlaught)
rownames(CTS) = seq(1,351,1)

data = list(M, FS, OFH, FC, CTS)
```


```{r}
mat_coldiff = function(X){
  
  p = length(X[1,]) # la dimension des individus
  M = diag(p)/p
  
  Pm = X %*% M %*% t(X)
  
  return(Pm)
  
}
```
